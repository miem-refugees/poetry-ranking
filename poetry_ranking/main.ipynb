{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {},
  "cells": [
    {
      "metadata": {},
      "source": [
        "import gc\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import textdescriptives as td\n",
        "import textstat\n",
        "from catboost import CatBoostRegressor\n",
        "from pandarallel import pandarallel\n",
        "from scipy.stats import kendalltau, spearmanr\n",
        "from sklearn.metrics import root_mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from textacy.text_stats import basics, counts, diversity, readability\n",
        "from tqdm import tqdm\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import pipeline\n",
        "\n",
        "from poetry_ranking.utils import get_sentence_embedding_sbert\n",
        "\n",
        "pandarallel.initialize(progress_bar=True, nb_workers=8)\n",
        "spacy_nlp = spacy.load(\"ru_core_news_lg\")\n",
        "textstat.set_lang(\"ru\")"
      ],
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading models..."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK\n",
            "INFO: Pandarallel will run on 8 workers.\n",
            "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/seara/Desktop/Github/poetry-ranking/.venv/lib/python3.11/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'ru_core_news_lg' (3.8.0) was trained with spaCy v3.8.0 and may not be 100% compatible with the current version (3.7.5). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  warnings.warn(warn_msg)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "def get_textstat_features(text: str):\n",
        "    textstat_features = {\n",
        "        f\"textstat_{key}\": value\n",
        "        for key, value in {\n",
        "            \"flesch_reading_ease\": textstat.flesch_reading_ease(text),\n",
        "            \"flesch_kincaid_grade\": textstat.flesch_kincaid_grade(text),\n",
        "            \"gunning_fog\": textstat.gunning_fog(text),\n",
        "            \"smog_index\": textstat.smog_index(text),\n",
        "            \"automated_readability_index\": textstat.automated_readability_index(text),\n",
        "            \"coleman_liau_index\": textstat.coleman_liau_index(text),\n",
        "            \"linsear_write_formula\": textstat.linsear_write_formula(text),\n",
        "            \"dale_chall_readability_score\": textstat.dale_chall_readability_score(text),\n",
        "            \"text_standard\": textstat.text_standard(text),\n",
        "            \"spache_readability\": textstat.spache_readability(text),\n",
        "            \"mcalpine_eflaw\": textstat.mcalpine_eflaw(text),\n",
        "            \"reading_time\": textstat.reading_time(text, ms_per_char=14.69),\n",
        "            \"syllable_count\": textstat.syllable_count(text),\n",
        "            \"lexicon_count\": textstat.lexicon_count(text, removepunct=True),\n",
        "            \"sentence_count\": textstat.sentence_count(text),\n",
        "            \"char_count\": textstat.char_count(text, ignore_spaces=True),\n",
        "            \"letter_count\": textstat.letter_count(text, ignore_spaces=True),\n",
        "            \"polysyllabcount\": textstat.polysyllabcount(text),\n",
        "            \"monosyllabcount\": textstat.monosyllabcount(text),\n",
        "            \"difficult_words\": textstat.difficult_words(text),\n",
        "        }.items()\n",
        "    }\n",
        "    return textstat_features\n",
        "\n",
        "\n",
        "def get_textacy_features(text: str):\n",
        "    doc = spacy_nlp(text)\n",
        "\n",
        "    stats = {\n",
        "        # Basic Statistics\n",
        "        \"n_sents\": basics.n_sents(doc),\n",
        "        \"n_words\": basics.n_words(doc),\n",
        "        \"n_unique_words\": basics.n_unique_words(doc),\n",
        "        \"n_chars_per_word_mean\": np.mean(basics.n_chars_per_word(doc)).item(),\n",
        "        \"n_chars\": basics.n_chars(doc),\n",
        "        \"n_long_words\": basics.n_long_words(doc),\n",
        "        \"n_syllables_per_word_mean\": np.mean(basics.n_syllables_per_word(doc)).item(),\n",
        "        \"n_syllables\": basics.n_syllables(doc),\n",
        "        \"n_monosyllable_words\": basics.n_monosyllable_words(doc),\n",
        "        \"n_polysyllable_words\": basics.n_polysyllable_words(doc),\n",
        "        \"entropy\": basics.entropy(doc),\n",
        "        # Readability Measures\n",
        "        \"readability_automated_readability_index\": readability.automated_readability_index(\n",
        "            doc\n",
        "        ),\n",
        "        \"readability_coleman_liau_index\": readability.coleman_liau_index(doc),\n",
        "        \"readability_flesch_kincaid_grade_level\": readability.flesch_kincaid_grade_level(\n",
        "            doc\n",
        "        ),\n",
        "        \"readability_flesch_reading_ease\": readability.flesch_reading_ease(doc),\n",
        "        \"readability_gunning_fog_index\": readability.gunning_fog_index(doc),\n",
        "        \"readability_smog_index\": readability.smog_index(doc),\n",
        "        \"readability_lix\": readability.lix(doc),\n",
        "        # Diversity Measures\n",
        "        \"diversity_ttr\": diversity.ttr(doc),\n",
        "        \"diversity_log_ttr\": diversity.log_ttr(doc),\n",
        "        \"diversity_segmented_ttr\": diversity.segmented_ttr(doc),\n",
        "        \"diversity_mtld\": diversity.mtld(doc),\n",
        "        \"diversity_hdd\": diversity.hdd(doc).item(),\n",
        "        # Counts\n",
        "        \"morph_counts\": counts.morph(doc),\n",
        "        \"pos_counts\": counts.pos(doc),\n",
        "        \"tag_counts\": counts.tag(doc),\n",
        "        \"dep_counts\": counts.dep(doc),\n",
        "    }\n",
        "\n",
        "    for category, counts_ in stats[\"morph_counts\"].items():\n",
        "        for subcategory, count in counts_.items():\n",
        "            stats[f\"morph_counts_{category}_{subcategory}\"] = count\n",
        "\n",
        "    # Process pos_counts\n",
        "    for pos, count in stats[\"pos_counts\"].items():\n",
        "        stats[f\"pos_counts_{pos}\"] = count\n",
        "\n",
        "    # Process tag_counts\n",
        "    for tag, count in stats[\"tag_counts\"].items():\n",
        "        stats[f\"tag_counts_{tag}\"] = count\n",
        "\n",
        "    # Process dep_counts\n",
        "    for dep, count in stats[\"dep_counts\"].items():\n",
        "        stats[f\"dep_counts_{dep}\"] = count\n",
        "\n",
        "    stats.pop(\"morph_counts\")\n",
        "    stats.pop(\"pos_counts\")\n",
        "    stats.pop(\"tag_counts\")\n",
        "    stats.pop(\"dep_counts\")\n",
        "\n",
        "    textacy_features = {f\"textacy_{key}\": value for key, value in stats.items()}\n",
        "\n",
        "    return textacy_features\n",
        "\n",
        "\n",
        "def make_features(df):\n",
        "    df = df.copy()\n",
        "\n",
        "    df_td = td.extract_metrics(text=df[\"output_text\"], lang=\"ru\", metrics=None)\n",
        "    df_td = df_td.add_prefix(\"textdescriptives_\")\n",
        "    df = pd.concat([df, df_td], axis=1)\n",
        "\n",
        "    df_textstat = df[\"output_text\"].parallel_apply(get_textstat_features).apply(pd.Series)\n",
        "    df = pd.concat([df, df_textstat], axis=1)\n",
        "\n",
        "    df_textacy = df[\"output_text\"].parallel_apply(get_textacy_features).apply(pd.Series)\n",
        "    df = pd.concat([df, df_textacy], axis=1)\n",
        "\n",
        "    return df"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "regenerate = False\n",
        "\n",
        "if regenerate:\n",
        "    df_train = pd.read_csv(\"data/raw/poetry_data_train.zip\")\n",
        "    df_test = pd.read_csv(\"data/raw/poetry_data_test.zip\")\n",
        "\n",
        "    df_train_features = make_features(df_train)\n",
        "    df_train_features.to_csv(\n",
        "        \"data/text_features/df_train_text_features.zip\",\n",
        "        index=False,\n",
        "        compression={\n",
        "            \"method\": \"zip\",\n",
        "            \"compresslevel\": 9,\n",
        "            \"archive_name\": \"df_train_text_features.csv\",\n",
        "        },\n",
        "    )\n",
        "\n",
        "    df_test_features = make_features(df_test)\n",
        "    df_test_features.to_csv(\n",
        "        \"data/text_features/df_test_text_features.zip\",\n",
        "        index=False,\n",
        "        compression={\n",
        "            \"method\": \"zip\",\n",
        "            \"compresslevel\": 9,\n",
        "            \"archive_name\": \"df_test_text_features.csv\",\n",
        "        },\n",
        "    )\n",
        "\n",
        "else:\n",
        "    df_train_features = pd.read_csv(\"data/text_features/df_train_text_features.zip\")\n",
        "    df_test_features = pd.read_csv(\"data/text_features/df_test_text_features.zip\")"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "regenerate_embeddings = False\n",
        "if regenerate_embeddings:\n",
        "    tqdm.pandas()\n",
        "\n",
        "    df_train_embeddings = pd.read_csv(\"data/raw/poetry_data_train.zip\")[[\"output_text\"]]\n",
        "    df_test_embeddings = pd.read_csv(\"data/raw/poetry_data_test.zip\")[[\"output_text\"]]\n",
        "\n",
        "    df_train_embeddings = df_train_embeddings.add_prefix(\"sbert_embedding_\")\n",
        "    df_test_embeddings = df_test_embeddings.add_prefix(\"sbert_embedding_\")\n",
        "\n",
        "    df_train_embeddings[\"sbert_embeddings\"] = df_train_embeddings[\n",
        "        \"sbert_embedding_output_text\"\n",
        "    ].progress_apply(lambda x: get_sentence_embedding_sbert(x).tolist())\n",
        "    df_train_embeddings.to_csv(\n",
        "        \"data/bert_embeddings/df_train_sbert_embeddings.zip\",\n",
        "        index=False,\n",
        "        compression={\n",
        "            \"method\": \"zip\",\n",
        "            \"compresslevel\": 9,\n",
        "            \"archive_name\": \"df_train_sbert_embeddings.csv\",\n",
        "        },\n",
        "    )\n",
        "\n",
        "    df_test_embeddings[\"sbert_embeddings\"] = df_test_embeddings[\n",
        "        \"sbert_embedding_output_text\"\n",
        "    ].progress_apply(lambda x: get_sentence_embedding_sbert(x).tolist())\n",
        "    df_test_embeddings.to_csv(\n",
        "        \"data/bert_embeddings/df_test_sbert_embeddings.zip\",\n",
        "        index=False,\n",
        "        compression={\n",
        "            \"method\": \"zip\",\n",
        "            \"compresslevel\": 9,\n",
        "            \"archive_name\": \"df_test_sbert_embeddings.csv\",\n",
        "        },\n",
        "    )\n",
        "else:\n",
        "    df_train_embeddings = pd.read_csv(\n",
        "        \"data/bert_embeddings/df_train_sbert_embeddings.zip\"\n",
        "    )\n",
        "    df_test_embeddings = pd.read_csv(\"data/bert_embeddings/df_test_sbert_embeddings.zip\")"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "regenerate_sentiment = False\n",
        "\n",
        "if regenerate_sentiment:\n",
        "    tqdm.pandas()\n",
        "    sentiment_model = pipeline(\n",
        "        model=\"seara/rubert-tiny2-russian-sentiment\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        device=\"cuda\",\n",
        "    )\n",
        "\n",
        "    df_train_sentiment = pd.read_csv(\"data/raw/poetry_data_train.zip\")[[\"output_text\"]]\n",
        "    df_test_sentiment = pd.read_csv(\"data/raw/poetry_data_test.zip\")[[\"output_text\"]]\n",
        "\n",
        "    df_train_sentiment = df_train_sentiment.add_prefix(\"sentiment_\")\n",
        "    df_test_sentiment = df_test_sentiment.add_prefix(\"sentiment_\")\n",
        "\n",
        "    def get_sentiment(text):\n",
        "        result = sentiment_model(text)[0]\n",
        "        return result[\"label\"]\n",
        "\n",
        "    df_train_sentiment[\"sentiment\"] = df_train_sentiment[\n",
        "        \"sentiment_output_text\"\n",
        "    ].progress_apply(lambda x: get_sentiment(x))\n",
        "\n",
        "    df_train_sentiment.to_csv(\n",
        "        \"data/sentiment/df_train_sentiment.zip\",\n",
        "        index=False,\n",
        "        compression={\n",
        "            \"method\": \"zip\",\n",
        "            \"compresslevel\": 9,\n",
        "            \"archive_name\": \"df_train_sentiment.csv\",\n",
        "        },\n",
        "    )\n",
        "\n",
        "    df_test_sentiment[\"sentiment\"] = df_test_sentiment[\n",
        "        \"sentiment_output_text\"\n",
        "    ].progress_apply(lambda x: get_sentiment(x))\n",
        "\n",
        "    df_test_sentiment.to_csv(\n",
        "        \"data/sentiment/df_test_sentiment.zip\",\n",
        "        index=False,\n",
        "        compression={\n",
        "            \"method\": \"zip\",\n",
        "            \"compresslevel\": 9,\n",
        "            \"archive_name\": \"df_test_sentiment.csv\",\n",
        "        },\n",
        "    )\n",
        "else:\n",
        "    df_train_sentiment = pd.read_csv(\"data/sentiment/df_train_sentiment.zip\")\n",
        "    df_test_sentiment = pd.read_csv(\"data/sentiment/df_test_sentiment.zip\")"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "df_train = pd.concat([df_train_features, df_train_embeddings, df_train_sentiment], axis=1)\n",
        "df_test = pd.concat([df_test_features, df_test_embeddings, df_test_sentiment], axis=1)\n",
        "\n",
        "\n",
        "del df_train_features\n",
        "del df_train_embeddings\n",
        "del df_train_sentiment\n",
        "\n",
        "del df_test_features\n",
        "del df_test_embeddings\n",
        "del df_test_sentiment\n",
        "\n",
        "gc.collect()"
      ],
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "228"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "def downcast_datatypes(df, cat_cols):\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print(\"Memory consumed (in MB) before downsizing: \", start_mem)\n",
        "\n",
        "    fcols = df.select_dtypes(\"float\").columns\n",
        "    icols = df.select_dtypes(\"integer\").columns\n",
        "\n",
        "    df[fcols] = df[fcols].apply(pd.to_numeric, downcast=\"float\")\n",
        "    df[icols] = df[icols].apply(pd.to_numeric, downcast=\"integer\")\n",
        "\n",
        "    for col in cat_cols:\n",
        "        df[col] = df[col].astype(\"category\")\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print(\"Memory consumed (in MB) after downsizing: \", end_mem)\n",
        "    print(f\"Memory usage decreased by ({100 * (start_mem - end_mem) / start_mem:.1f}% )\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "df_train = downcast_datatypes(df_train, cat_cols=[\"genre\", \"sentiment\"])\n",
        "df_test = downcast_datatypes(df_test, cat_cols=[\"genre\", \"sentiment\"])"
      ],
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory consumed (in MB) before downsizing:  251.43099975585938\n",
            "Memory consumed (in MB) after downsizing:  125.25503158569336\n",
            "Memory usage decreased by (50.2% )\n",
            "Memory consumed (in MB) before downsizing:  62.85784435272217\n",
            "Memory consumed (in MB) after downsizing:  31.215909957885742\n",
            "Memory usage decreased by (50.3% )\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "def preprocess_df(df, predict=False):\n",
        "    if not predict:\n",
        "        df = df.dropna(axis=1)\n",
        "        df = df.loc[:, df.nunique() > 2]\n",
        "        df = df.drop(\"url\", axis=1, errors=\"ignore\")\n",
        "    df[\"textstat_text_standard\"] = (\n",
        "        df[\"textstat_text_standard\"]\n",
        "        .str.extractall(r\"(\\d+)\")\n",
        "        .unstack()\n",
        "        .astype(float)\n",
        "        .mean(axis=1)\n",
        "    )\n",
        "\n",
        "    if not predict:\n",
        "        df = df.drop(\"textdescriptives_passed_quality_check\", axis=1, errors=\"ignore\")\n",
        "        temp_genre_value_counts = df[\"genre\"].value_counts()\n",
        "        df[\"genre\"] = df[\"genre\"].apply(\n",
        "            lambda x: x if temp_genre_value_counts[x] >= 50 else \"\u0414\u0440\u0443\u0433\u043e\u0435\"\n",
        "        )\n",
        "        df = df.drop(\n",
        "            [\"sbert_embedding_output_text\", \"sentiment_output_text\"],\n",
        "            axis=1,\n",
        "            errors=\"ignore\",\n",
        "        )\n",
        "        df = df.T.drop_duplicates().T\n",
        "\n",
        "        df[\"sbert_embeddings\"] = df[\"sbert_embeddings\"].parallel_apply(\n",
        "            lambda x: eval(x)[0]\n",
        "        )\n",
        "\n",
        "    if predict:\n",
        "        df[\"sbert_embeddings\"] = df[\"sbert_embeddings\"].parallel_apply(lambda x: x[0])\n",
        "\n",
        "    embedding_df = pd.DataFrame(df[\"sbert_embeddings\"].tolist(), index=df.index)\n",
        "\n",
        "    embedding_df.columns = [f\"embedding_{i+1}\" for i in range(embedding_df.shape[1])]\n",
        "\n",
        "    df = df.drop(columns=[\"sbert_embeddings\"]).join(embedding_df)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "df_train = preprocess_df(df_train)\n",
        "df_test = preprocess_df(df_test)[df_train.columns]"
      ],
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d510d71d78a34c7d864eace46e051bb7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=17300), Label(value='0 / 17300')))\u2026"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d69acf617af24e14bcfec8c9e866608d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=4325), Label(value='0 / 4325'))), \u2026"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "def print_metrics(model, X_val, y_val, prefix):\n",
        "    y_pred = model.predict(X_val)\n",
        "    mse = root_mean_squared_error(y_val, y_pred)\n",
        "    print(f\"{prefix.upper()}: Root mean Squared Error: {mse}\")\n",
        "    kendall_tau, _ = kendalltau(y_val, y_pred)\n",
        "    print(f\"{prefix.upper()}: Kendall's Tau: {kendall_tau}\")\n",
        "    kendall_tau, _ = spearmanr(y_val, y_pred)\n",
        "    print(f\"{prefix.upper()}: Spearmanr: {kendall_tau}\")\n",
        "\n",
        "\n",
        "class KendallTauMetric:\n",
        "    def get_final_error(self, error, weight):\n",
        "        return error / (weight + 1e-38)\n",
        "\n",
        "    def is_max_optimal(self):\n",
        "        return True\n",
        "\n",
        "    def evaluate(self, approxes, target, weight):\n",
        "        assert len(approxes) == 1\n",
        "        assert len(target) == len(approxes[0])\n",
        "        approx = approxes[0]\n",
        "        tau, _ = kendalltau(target, approx)\n",
        "        error_sum = tau\n",
        "        weight_sum = 1.0\n",
        "        return error_sum, weight_sum"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "X = df_train.drop([\"views\", \"genre\", \"rating\"], axis=1)\n",
        "y = df_train[\"rating\"]\n",
        "\n",
        "X_test = df_test.drop([\"views\", \"genre\", \"rating\"], axis=1)\n",
        "y_test = df_test[\"rating\"]\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "top_model = CatBoostRegressor(eval_metric=KendallTauMetric(), verbose=0)\n",
        "top_model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    eval_set=(X_val, y_val),\n",
        "    text_features=[\"output_text\"],\n",
        "    cat_features=[\"sentiment\"],\n",
        "    early_stopping_rounds=50,\n",
        ")\n",
        "\n",
        "print_metrics(top_model, X_val, y_val, prefix=\"val\")\n",
        "print_metrics(top_model, X_test, y_test, prefix=\"test\")"
      ],
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/seara/Desktop/Github/poetry-ranking/.venv/lib/python3.11/site-packages/catboost/core.py:2321: UserWarning: Failed to import numba for optimizing custom metrics and objectives\n",
            "  _check_train_params(params)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VAL: Root mean Squared Error: 31.259051561009013\n",
            "VAL: Kendall's Tau: 0.20418599881519792\n",
            "VAL: Spearmanr: 0.27505569185643514\n",
            "TEST: Root mean Squared Error: 32.82181614756902\n",
            "TEST: Kendall's Tau: 0.2064323571740577\n",
            "TEST: Spearmanr: 0.27833595332250965\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "X = df_train.drop([\"views\", \"genre\", \"rating\"], axis=1)\n",
        "y = df_train[\"views\"]\n",
        "\n",
        "X_test = df_test.drop([\"views\", \"genre\", \"rating\"], axis=1)\n",
        "y_test = df_test[\"views\"]\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = CatBoostRegressor(eval_metric=KendallTauMetric(), verbose=0)\n",
        "model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    eval_set=(X_val, y_val),\n",
        "    text_features=[\"output_text\"],\n",
        "    cat_features=[\"sentiment\"],\n",
        "    early_stopping_rounds=50,\n",
        ")\n",
        "print_metrics(model, X_val, y_val, prefix=\"val\")\n",
        "print_metrics(model, X_test, y_test, prefix=\"test\")"
      ],
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/seara/Desktop/Github/poetry-ranking/.venv/lib/python3.11/site-packages/catboost/core.py:2321: UserWarning: Failed to import numba for optimizing custom metrics and objectives\n",
            "  _check_train_params(params)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VAL: Root mean Squared Error: 389.95554565117465\n",
            "VAL: Kendall's Tau: 0.0935416419111623\n",
            "VAL: Spearmanr: 0.13935603376280564\n",
            "TEST: Root mean Squared Error: 394.55398695048194\n",
            "TEST: Kendall's Tau: 0.10004089731549336\n",
            "TEST: Spearmanr: 0.14882060984358098\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "def rating_views_target(rating, views):\n",
        "    if views == 0:\n",
        "        return 0\n",
        "    return views + np.exp(1 + rating / (rating + views)) * rating\n",
        "\n",
        "\n",
        "X = df_train.drop([\"views\", \"genre\", \"rating\"], axis=1)\n",
        "y = df_train.apply(lambda row: rating_views_target(row[\"rating\"], row[\"views\"]), axis=1)\n",
        "\n",
        "X_test = df_test.drop([\"views\", \"genre\", \"rating\"], axis=1)\n",
        "y_test = df_test.apply(\n",
        "    lambda row: rating_views_target(row[\"rating\"], row[\"views\"]), axis=1\n",
        ")\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = CatBoostRegressor(eval_metric=KendallTauMetric(), verbose=0)\n",
        "model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    eval_set=(X_val, y_val),\n",
        "    text_features=[\"output_text\"],\n",
        "    cat_features=[\"sentiment\"],\n",
        "    early_stopping_rounds=50,\n",
        ")\n",
        "print_metrics(model, X_val, y_val, prefix=\"val\")\n",
        "print_metrics(model, X_test, y_test, prefix=\"test\")"
      ],
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/seara/Desktop/Github/poetry-ranking/.venv/lib/python3.11/site-packages/catboost/core.py:2321: UserWarning: Failed to import numba for optimizing custom metrics and objectives\n",
            "  _check_train_params(params)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VAL: Root mean Squared Error: 426.7664170778184\n",
            "VAL: Kendall's Tau: 0.1097880220287051\n",
            "VAL: Spearmanr: 0.16429213197740228\n",
            "TEST: Root mean Squared Error: 433.2314645951856\n",
            "TEST: Kendall's Tau: 0.11153520708407551\n",
            "TEST: Spearmanr: 0.16677947306110338\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "importances = top_model.get_feature_importance()\n",
        "feature_importance_df = pd.DataFrame(\n",
        "    {\"Feature\": X_train.columns, \"Importance\": importances}\n",
        ").sort_values(by=\"Importance\", ascending=False)\n",
        "feature_importance_df.head(10)"
      ],
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Feature</th>\n",
              "      <th>Importance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>output_text</td>\n",
              "      <td>29.473032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>textstat_difficult_words</td>\n",
              "      <td>2.356093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>textacy_n_chars</td>\n",
              "      <td>1.588140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>embedding_71</td>\n",
              "      <td>1.399563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>textstat_letter_count</td>\n",
              "      <td>1.339400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245</th>\n",
              "      <td>embedding_141</td>\n",
              "      <td>1.219181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>textdescriptives_top_ngram_chr_fraction_2</td>\n",
              "      <td>1.204721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>737</th>\n",
              "      <td>embedding_633</td>\n",
              "      <td>1.100763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>textstat_sentence_count</td>\n",
              "      <td>1.086004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>sentiment</td>\n",
              "      <td>1.000711</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       Feature  Importance\n",
              "0                                  output_text   29.473032\n",
              "79                    textstat_difficult_words    2.356093\n",
              "83                             textacy_n_chars    1.588140\n",
              "175                               embedding_71    1.399563\n",
              "76                       textstat_letter_count    1.339400\n",
              "245                              embedding_141    1.219181\n",
              "45   textdescriptives_top_ngram_chr_fraction_2    1.204721\n",
              "737                              embedding_633    1.100763\n",
              "74                     textstat_sentence_count    1.086004\n",
              "104                                  sentiment    1.000711"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "pushkin = \"\"\"\u00ab\u041c\u043e\u0439 \u0434\u044f\u0434\u044f \u0441\u0430\u043c\u044b\u0445 \u0447\u0435\u0441\u0442\u043d\u044b\u0445 \u043f\u0440\u0430\u0432\u0438\u043b,\n",
        "\u041a\u043e\u0433\u0434\u0430 \u043d\u0435 \u0432 \u0448\u0443\u0442\u043a\u0443 \u0437\u0430\u043d\u0435\u043c\u043e\u0433,\n",
        "\u041e\u043d \u0443\u0432\u0430\u0436\u0430\u0442\u044c \u0441\u0435\u0431\u044f \u0437\u0430\u0441\u0442\u0430\u0432\u0438\u043b\n",
        "\u0418 \u043b\u0443\u0447\u0448\u0435 \u0432\u044b\u0434\u0443\u043c\u0430\u0442\u044c \u043d\u0435 \u043c\u043e\u0433.\n",
        "\u0415\u0433\u043e \u043f\u0440\u0438\u043c\u0435\u0440 \u0434\u0440\u0443\u0433\u0438\u043c \u043d\u0430\u0443\u043a\u0430;\n",
        "\u041d\u043e, \u0431\u043e\u0436\u0435 \u043c\u043e\u0439, \u043a\u0430\u043a\u0430\u044f \u0441\u043a\u0443\u043a\u0430\n",
        "\u0421 \u0431\u043e\u043b\u044c\u043d\u044b\u043c \u0441\u0438\u0434\u0435\u0442\u044c \u0438 \u0434\u0435\u043d\u044c \u0438 \u043d\u043e\u0447\u044c,\n",
        "\u041d\u0435 \u043e\u0442\u0445\u043e\u0434\u044f \u043d\u0438 \u0448\u0430\u0433\u0443 \u043f\u0440\u043e\u0447\u044c!\n",
        "\u041a\u0430\u043a\u043e\u0435 \u043d\u0438\u0437\u043a\u043e\u0435 \u043a\u043e\u0432\u0430\u0440\u0441\u0442\u0432\u043e\n",
        "\u041f\u043e\u043b\u0443\u0436\u0438\u0432\u043e\u0433\u043e \u0437\u0430\u0431\u0430\u0432\u043b\u044f\u0442\u044c,\n",
        "\u0415\u043c\u0443 \u043f\u043e\u0434\u0443\u0448\u043a\u0438 \u043f\u043e\u043f\u0440\u0430\u0432\u043b\u044f\u0442\u044c,\n",
        "\u041f\u0435\u0447\u0430\u043b\u044c\u043d\u043e \u043f\u043e\u0434\u043d\u043e\u0441\u0438\u0442\u044c \u043b\u0435\u043a\u0430\u0440\u0441\u0442\u0432\u043e,\n",
        "\u0412\u0437\u0434\u044b\u0445\u0430\u0442\u044c \u0438 \u0434\u0443\u043c\u0430\u0442\u044c \u043f\u0440\u043e \u0441\u0435\u0431\u044f:\n",
        "\u041a\u043e\u0433\u0434\u0430 \u0436\u0435 \u0447\u0435\u0440\u0442 \u0432\u043e\u0437\u044c\u043c\u0435\u0442 \u0442\u0435\u0431\u044f!\u00bb\"\"\"\n",
        "\n",
        "esenin = \"\"\"\u0417\u0430\u043c\u0435\u0442\u0430\u043b\u0441\u044f \u043f\u043e\u0436\u0430\u0440 \u0433\u043e\u043b\u0443\u0431\u043e\u0439,\n",
        "\u041f\u043e\u0437\u0430\u0431\u044b\u043b\u0438\u0441\u044c \u0440\u043e\u0434\u0438\u043c\u044b\u0435 \u0434\u0430\u043b\u0438.\n",
        "\u0412 \u043f\u0435\u0440\u0432\u044b\u0439 \u0440\u0430\u0437 \u044f \u0437\u0430\u043f\u0435\u043b \u043f\u0440\u043e \u043b\u044e\u0431\u043e\u0432\u044c,\n",
        "\u0412 \u043f\u0435\u0440\u0432\u044b\u0439 \u0440\u0430\u0437 \u043e\u0442\u0440\u0435\u043a\u0430\u044e\u0441\u044c \u0441\u043a\u0430\u043d\u0434\u0430\u043b\u0438\u0442\u044c.\n",
        "\u0411\u044b\u043b \u044f \u0432\u0435\u0441\u044c \u2014 \u043a\u0430\u043a \u0437\u0430\u043f\u0443\u0449\u0435\u043d\u043d\u044b\u0439 \u0441\u0430\u0434,\n",
        "\u0411\u044b\u043b \u043d\u0430 \u0436\u0435\u043d\u0449\u0438\u043d \u0438 \u0437\u0435\u043b\u0438\u0435 \u043f\u0430\u0434\u043a\u0438\u0439.\n",
        "\u0420\u0430\u0437\u043e\u043d\u0440\u0430\u0432\u0438\u043b\u043e\u0441\u044c \u043f\u0438\u0442\u044c \u0438 \u043f\u043b\u044f\u0441\u0430\u0442\u044c\n",
        "\u0418 \u0442\u0435\u0440\u044f\u0442\u044c \u0441\u0432\u043e\u044e \u0436\u0438\u0437\u043d\u044c \u0431\u0435\u0437 \u043e\u0433\u043b\u044f\u0434\u043a\u0438.\n",
        "\u041c\u043d\u0435 \u0431\u044b \u0442\u043e\u043b\u044c\u043a\u043e \u0441\u043c\u043e\u0442\u0440\u0435\u0442\u044c \u043d\u0430 \u0442\u0435\u0431\u044f,\n",
        "\u0412\u0438\u0434\u0435\u0442\u044c \u0433\u043b\u0430\u0437 \u0437\u043b\u0430\u0442\u043e-\u043a\u0430\u0440\u0438\u0439 \u043e\u043c\u0443\u0442,\n",
        "\u0418 \u0447\u0442\u043e\u0431, \u043f\u0440\u043e\u0448\u043b\u043e\u0435 \u043d\u0435 \u043b\u044e\u0431\u044f,\n",
        "\u0422\u044b \u0443\u0439\u0442\u0438 \u043d\u0435 \u0441\u043c\u043e\u0433\u043b\u0430 \u043a \u0434\u0440\u0443\u0433\u043e\u043c\u0443.\"\"\"\n",
        "\n",
        "gpt_random = \"\"\"\u0421\u043a\u0430\u0447\u0435\u0442 \u0432\u043e\u0440\u043e\u0431\u0435\u0439 \u043d\u0430 \u043b\u0443\u0436\u0443,\n",
        "\u0421\u044b\u0440 \u0441 \u043e\u0440\u0435\u0445\u043e\u043c \u0432 \u0442\u0451\u043c\u043d\u043e\u043c \u0441\u0442\u0443\u0436\u0435.\n",
        "\u0422\u0440\u0438 \u043a\u043e\u043b\u0435\u0441\u0430 \u043f\u043e \u043b\u0443\u0436\u0430\u0439\u043a\u0435 \u043f\u043b\u044b\u0432\u0443\u0442,\n",
        "\u0410 \u0432 \u0433\u043e\u0440\u043b\u0435 \u0439\u043e\u0433\u0443\u0440\u0442, \u0430 \u0440\u044f\u0434\u043e\u043c \u0441\u0430\u043b\u044e\u0442.\n",
        "\n",
        "\u041b\u0438\u043f\u043d\u0435\u0442 \u043a\u043b\u044e\u043a\u0432\u0430 \u043a \u043f\u043e\u0442\u043e\u043b\u043a\u0443,\n",
        "\u0416\u0443\u043a \u0442\u0430\u043d\u0446\u0443\u0435\u0442 \u0432 \u043c\u043e\u043b\u043e\u043a\u0443,\n",
        "\u0412 \u043d\u0435\u0431\u0435 \u0440\u0430\u0434\u0443\u0433\u0430-\u0442\u0435\u0442\u0440\u0430\u0434\u044c,\n",
        "\u041a\u0430\u043a \u0436\u0435 \u0442\u0443\u0442 \u043d\u0435 \u043f\u043e\u043b\u0435\u0442\u0430\u0442\u044c?\"\"\"\n",
        "\n",
        "top_poem_in_test = df_test[df_test[\"rating\"] == df_test[\"rating\"].max()][\n",
        "    \"output_text\"\n",
        "].values[0]\n",
        "\n",
        "bottom_poem_in_test = df_test[df_test[\"views\"] == df_test[\"views\"].min()][\n",
        "    \"output_text\"\n",
        "].values[1]\n",
        "\n",
        "random_words = \"\"\"\u041e\u0437\u0435\u0440\u043e\n",
        "\u041f\u0443\u0442\u0435\u0448\u0435\u0441\u0442\u0432\u0438\u0435\n",
        "\u0421\u0438\u044f\u043d\u0438\u0435\n",
        "\u041b\u0438\u0441\u0442\u043e\u043f\u0430\u0434\n",
        "\u0412\u0435\u043b\u043e\u0441\u0438\u043f\u0435\u0434\n",
        "\u041c\u0435\u0447\u0442\u0430\n",
        "\u0417\u043e\u043d\u0442\u0438\u043a\n",
        "\u0420\u0435\u043a\u0430\n",
        "\u0413\u043e\u0440\u0438\u0437\u043e\u043d\u0442\n",
        "\u0412\u0434\u043e\u0445\u043d\u043e\u0432\u0435\u043d\u0438\u0435\n",
        "\u0424\u043e\u043d\u0430\u0440\u044c\n",
        "\u041b\u0430\u0441\u0442\u043e\u0447\u043a\u0430\n",
        "\u0421\u0442\u0443\u043f\u0435\u043d\u044c\u043a\u0430\"\"\"\n",
        "\n",
        "sanity_check = [\n",
        "    pushkin,\n",
        "    esenin,\n",
        "    gpt_random,\n",
        "    top_poem_in_test,\n",
        "    bottom_poem_in_test,\n",
        "    random_words,\n",
        "]\n",
        "\n",
        "\n",
        "def rank_poems(model, poems: list[str]) -> list[int]:\n",
        "\n",
        "    df = pd.DataFrame({\"output_text\": poems})\n",
        "\n",
        "    df = make_features(df)\n",
        "    df[\"sbert_embeddings\"] = df[\"output_text\"].apply(\n",
        "        lambda x: get_sentence_embedding_sbert(x).tolist()\n",
        "    )\n",
        "\n",
        "    sentiment_model = pipeline(\n",
        "        model=\"seara/rubert-tiny2-russian-sentiment\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        device=\"cpu\",\n",
        "    )\n",
        "\n",
        "    def get_sentiment(text):\n",
        "        result = sentiment_model(text)[0]\n",
        "        return result[\"label\"]\n",
        "\n",
        "    df[\"sentiment\"] = df[\"output_text\"].apply(lambda x: get_sentiment(x))\n",
        "\n",
        "    df = downcast_datatypes(df, cat_cols=[\"sentiment\"])\n",
        "\n",
        "    print(df.columns.to_list())\n",
        "\n",
        "    print(X_train.columns.to_list())\n",
        "\n",
        "    df = preprocess_df(df, predict=True)[X_train.columns]\n",
        "\n",
        "    return model.predict(df)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "ranks = rank_poems(top_model, sanity_check)"
      ],
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;4m\u2139 No spacy model provided. Inferring spacy model for ru.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/seara/Desktop/Github/poetry-ranking/.venv/lib/python3.11/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'ru_core_news_lg' (3.8.0) was trained with spaCy v3.8.0 and may not be 100% compatible with the current version (3.7.5). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;3m\u26a0 Could not load lexeme probability table for language ru. This will\n",
            "result in NaN values for perplexity and entropy.\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3df5c71142204f689d94e3000624f6ff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1), Label(value='0 / 1'))), HBox(c\u2026"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/seara/Desktop/Github/poetry-ranking/.venv/lib/python3.11/site-packages/textstat/textstat.py:1489: Warning: There is no easy words vocabulary for ru, using english.\n",
            "  warnings.warn(\n",
            "/home/seara/Desktop/Github/poetry-ranking/.venv/lib/python3.11/site-packages/textstat/textstat.py:1489: Warning: There is no easy words vocabulary for ru, using english.\n",
            "  warnings.warn(\n",
            "/home/seara/Desktop/Github/poetry-ranking/.venv/lib/python3.11/site-packages/textstat/textstat.py:1489: Warning: There is no easy words vocabulary for ru, using english.\n",
            "  warnings.warn(\n",
            "/home/seara/Desktop/Github/poetry-ranking/.venv/lib/python3.11/site-packages/textstat/textstat.py:1489: Warning: There is no easy words vocabulary for ru, using english.\n",
            "  warnings.warn(\n",
            "/home/seara/Desktop/Github/poetry-ranking/.venv/lib/python3.11/site-packages/textstat/textstat.py:1489: Warning: There is no easy words vocabulary for ru, using english.\n",
            "  warnings.warn(\n",
            "/home/seara/Desktop/Github/poetry-ranking/.venv/lib/python3.11/site-packages/textstat/textstat.py:1489: Warning: There is no easy words vocabulary for ru, using english.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "48907b77a7ac4c7eb4b6e081ae88826b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1), Label(value='0 / 1'))), HBox(c\u2026"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory consumed (in MB) before downsizing:  0.009698867797851562\n",
            "Memory consumed (in MB) after downsizing:  0.004932403564453125\n",
            "Memory usage decreased by (49.1% )\n",
            "['output_text', 'textdescriptives_text', 'textdescriptives_entropy', 'textdescriptives_perplexity', 'textdescriptives_per_word_perplexity', 'textdescriptives_dependency_distance_mean', 'textdescriptives_dependency_distance_std', 'textdescriptives_prop_adjacent_dependency_relation_mean', 'textdescriptives_prop_adjacent_dependency_relation_std', 'textdescriptives_flesch_reading_ease', 'textdescriptives_flesch_kincaid_grade', 'textdescriptives_smog', 'textdescriptives_gunning_fog', 'textdescriptives_automated_readability_index', 'textdescriptives_coleman_liau_index', 'textdescriptives_lix', 'textdescriptives_rix', 'textdescriptives_token_length_mean', 'textdescriptives_token_length_median', 'textdescriptives_token_length_std', 'textdescriptives_sentence_length_mean', 'textdescriptives_sentence_length_median', 'textdescriptives_sentence_length_std', 'textdescriptives_syllables_per_token_mean', 'textdescriptives_syllables_per_token_median', 'textdescriptives_syllables_per_token_std', 'textdescriptives_n_tokens', 'textdescriptives_n_unique_tokens', 'textdescriptives_proportion_unique_tokens', 'textdescriptives_n_characters', 'textdescriptives_n_sentences', 'textdescriptives_passed_quality_check', 'textdescriptives_n_stop_words', 'textdescriptives_alpha_ratio', 'textdescriptives_mean_word_length', 'textdescriptives_doc_length', 'textdescriptives_symbol_to_word_ratio_#', 'textdescriptives_proportion_ellipsis', 'textdescriptives_proportion_bullet_points', 'textdescriptives_contains_lorem ipsum', 'textdescriptives_duplicate_line_chr_fraction', 'textdescriptives_duplicate_paragraph_chr_fraction', 'textdescriptives_duplicate_ngram_chr_fraction_5', 'textdescriptives_duplicate_ngram_chr_fraction_6', 'textdescriptives_duplicate_ngram_chr_fraction_7', 'textdescriptives_duplicate_ngram_chr_fraction_8', 'textdescriptives_duplicate_ngram_chr_fraction_9', 'textdescriptives_duplicate_ngram_chr_fraction_10', 'textdescriptives_top_ngram_chr_fraction_2', 'textdescriptives_top_ngram_chr_fraction_3', 'textdescriptives_top_ngram_chr_fraction_4', 'textdescriptives_oov_ratio', 'textdescriptives_pos_prop_ADJ', 'textdescriptives_pos_prop_ADP', 'textdescriptives_pos_prop_ADV', 'textdescriptives_pos_prop_AUX', 'textdescriptives_pos_prop_CCONJ', 'textdescriptives_pos_prop_DET', 'textdescriptives_pos_prop_INTJ', 'textdescriptives_pos_prop_NOUN', 'textdescriptives_pos_prop_NUM', 'textdescriptives_pos_prop_PART', 'textdescriptives_pos_prop_PRON', 'textdescriptives_pos_prop_PROPN', 'textdescriptives_pos_prop_PUNCT', 'textdescriptives_pos_prop_SCONJ', 'textdescriptives_pos_prop_SYM', 'textdescriptives_pos_prop_VERB', 'textdescriptives_pos_prop_X', 'textdescriptives_first_order_coherence', 'textdescriptives_second_order_coherence', 'textstat_flesch_reading_ease', 'textstat_flesch_kincaid_grade', 'textstat_gunning_fog', 'textstat_smog_index', 'textstat_automated_readability_index', 'textstat_coleman_liau_index', 'textstat_linsear_write_formula', 'textstat_dale_chall_readability_score', 'textstat_text_standard', 'textstat_spache_readability', 'textstat_mcalpine_eflaw', 'textstat_reading_time', 'textstat_syllable_count', 'textstat_lexicon_count', 'textstat_sentence_count', 'textstat_char_count', 'textstat_letter_count', 'textstat_polysyllabcount', 'textstat_monosyllabcount', 'textstat_difficult_words', 'textacy_n_sents', 'textacy_n_words', 'textacy_n_unique_words', 'textacy_n_chars_per_word_mean', 'textacy_n_chars', 'textacy_n_long_words', 'textacy_n_syllables_per_word_mean', 'textacy_n_syllables', 'textacy_n_monosyllable_words', 'textacy_n_polysyllable_words', 'textacy_entropy', 'textacy_readability_automated_readability_index', 'textacy_readability_coleman_liau_index', 'textacy_readability_flesch_kincaid_grade_level', 'textacy_readability_flesch_reading_ease', 'textacy_readability_gunning_fog_index', 'textacy_readability_smog_index', 'textacy_readability_lix', 'textacy_diversity_ttr', 'textacy_diversity_log_ttr', 'textacy_diversity_segmented_ttr', 'textacy_diversity_mtld', 'textacy_diversity_hdd', 'textacy_morph_counts_Case_Nom', 'textacy_morph_counts_Case_Gen', 'textacy_morph_counts_Case_Acc', 'textacy_morph_counts_Case_Dat', 'textacy_morph_counts_Case_Voc', 'textacy_morph_counts_Case_Ins', 'textacy_morph_counts_Case_Par', 'textacy_morph_counts_Gender_Masc', 'textacy_morph_counts_Gender_Neut', 'textacy_morph_counts_Gender_Fem', 'textacy_morph_counts_Number_Sing', 'textacy_morph_counts_Number_Plur', 'textacy_morph_counts_Animacy_Anim', 'textacy_morph_counts_Animacy_Inan', 'textacy_morph_counts_Degree_Pos', 'textacy_morph_counts_Degree_Cmp', 'textacy_morph_counts_Polarity_Neg', 'textacy_morph_counts_Aspect_Perf', 'textacy_morph_counts_Aspect_Imp', 'textacy_morph_counts_Mood_Ind', 'textacy_morph_counts_Tense_Past', 'textacy_morph_counts_Tense_Pres', 'textacy_morph_counts_Tense_Fut', 'textacy_morph_counts_VerbForm_Fin', 'textacy_morph_counts_VerbForm_Inf', 'textacy_morph_counts_VerbForm_Conv', 'textacy_morph_counts_Voice_Act', 'textacy_morph_counts_Person_Third', 'textacy_morph_counts_Person_Second', 'textacy_pos_counts_PUNCT', 'textacy_pos_counts_DET', 'textacy_pos_counts_NOUN', 'textacy_pos_counts_ADJ', 'textacy_pos_counts_SPACE', 'textacy_pos_counts_SCONJ', 'textacy_pos_counts_PART', 'textacy_pos_counts_ADP', 'textacy_pos_counts_VERB', 'textacy_pos_counts_PRON', 'textacy_pos_counts_CCONJ', 'textacy_pos_counts_ADV', 'textacy_pos_counts_PROPN', 'textacy_tag_counts_PUNCT', 'textacy_tag_counts_DET', 'textacy_tag_counts_NOUN', 'textacy_tag_counts_ADJ', 'textacy_tag_counts_SPACE', 'textacy_tag_counts_SCONJ', 'textacy_tag_counts_PART', 'textacy_tag_counts_ADP', 'textacy_tag_counts_VERB', 'textacy_tag_counts_PRON', 'textacy_tag_counts_CCONJ', 'textacy_tag_counts_ADV', 'textacy_tag_counts_PROPN', 'textacy_dep_counts_punct', 'textacy_dep_counts_det', 'textacy_dep_counts_nsubj', 'textacy_dep_counts_amod', 'textacy_dep_counts_nmod', 'textacy_dep_counts_dep', 'textacy_dep_counts_mark', 'textacy_dep_counts_advmod', 'textacy_dep_counts_case', 'textacy_dep_counts_obl', 'textacy_dep_counts_acl:relcl', 'textacy_dep_counts_appos', 'textacy_dep_counts_xcomp', 'textacy_dep_counts_obj', 'textacy_dep_counts_ROOT', 'textacy_dep_counts_cc', 'textacy_dep_counts_conj', 'textacy_dep_counts_parataxis', 'textacy_dep_counts_iobj', 'textacy_dep_counts_ccomp', 'textacy_morph_counts_Mood_Cnd', 'textacy_morph_counts_VerbForm_Part', 'textacy_morph_counts_Voice_Mid', 'textacy_morph_counts_Voice_Pass', 'textacy_morph_counts_Person_First', 'textacy_pos_counts_AUX', 'textacy_tag_counts_AUX', 'textacy_dep_counts_cop', 'textacy_dep_counts_csubj', 'textacy_dep_counts_aux', 'textacy_morph_counts_Case_Loc', 'textacy_pos_counts_NUM', 'textacy_tag_counts_NUM', 'textacy_dep_counts_nummod:gov', 'textacy_dep_counts_advcl', 'textacy_morph_counts_StyleVariant_Short', 'textacy_dep_counts_acl', 'textacy_dep_counts_aux:pass', 'textacy_dep_counts_flat:name', 'sbert_embeddings', 'sentiment']\n",
            "['output_text', 'textdescriptives_pos_prop_ADJ', 'textdescriptives_pos_prop_ADP', 'textdescriptives_pos_prop_ADV', 'textdescriptives_pos_prop_AUX', 'textdescriptives_pos_prop_CCONJ', 'textdescriptives_pos_prop_DET', 'textdescriptives_pos_prop_INTJ', 'textdescriptives_pos_prop_NOUN', 'textdescriptives_pos_prop_NUM', 'textdescriptives_pos_prop_PART', 'textdescriptives_pos_prop_PRON', 'textdescriptives_pos_prop_PROPN', 'textdescriptives_pos_prop_PUNCT', 'textdescriptives_pos_prop_SCONJ', 'textdescriptives_pos_prop_SYM', 'textdescriptives_pos_prop_VERB', 'textdescriptives_pos_prop_X', 'textdescriptives_token_length_mean', 'textdescriptives_token_length_median', 'textdescriptives_token_length_std', 'textdescriptives_sentence_length_mean', 'textdescriptives_sentence_length_median', 'textdescriptives_sentence_length_std', 'textdescriptives_syllables_per_token_mean', 'textdescriptives_syllables_per_token_median', 'textdescriptives_syllables_per_token_std', 'textdescriptives_n_tokens', 'textdescriptives_n_unique_tokens', 'textdescriptives_proportion_unique_tokens', 'textdescriptives_n_characters', 'textdescriptives_n_sentences', 'textdescriptives_n_stop_words', 'textdescriptives_alpha_ratio', 'textdescriptives_mean_word_length', 'textdescriptives_doc_length', 'textdescriptives_proportion_ellipsis', 'textdescriptives_proportion_bullet_points', 'textdescriptives_duplicate_line_chr_fraction', 'textdescriptives_duplicate_ngram_chr_fraction_5', 'textdescriptives_duplicate_ngram_chr_fraction_6', 'textdescriptives_duplicate_ngram_chr_fraction_7', 'textdescriptives_duplicate_ngram_chr_fraction_8', 'textdescriptives_duplicate_ngram_chr_fraction_9', 'textdescriptives_duplicate_ngram_chr_fraction_10', 'textdescriptives_top_ngram_chr_fraction_2', 'textdescriptives_top_ngram_chr_fraction_3', 'textdescriptives_top_ngram_chr_fraction_4', 'textdescriptives_oov_ratio', 'textdescriptives_flesch_reading_ease', 'textdescriptives_flesch_kincaid_grade', 'textdescriptives_gunning_fog', 'textdescriptives_automated_readability_index', 'textdescriptives_coleman_liau_index', 'textdescriptives_lix', 'textdescriptives_rix', 'textdescriptives_dependency_distance_mean', 'textdescriptives_dependency_distance_std', 'textdescriptives_prop_adjacent_dependency_relation_mean', 'textdescriptives_prop_adjacent_dependency_relation_std', 'textstat_flesch_reading_ease', 'textstat_flesch_kincaid_grade', 'textstat_gunning_fog', 'textstat_smog_index', 'textstat_automated_readability_index', 'textstat_coleman_liau_index', 'textstat_linsear_write_formula', 'textstat_dale_chall_readability_score', 'textstat_text_standard', 'textstat_spache_readability', 'textstat_mcalpine_eflaw', 'textstat_reading_time', 'textstat_syllable_count', 'textstat_lexicon_count', 'textstat_sentence_count', 'textstat_char_count', 'textstat_letter_count', 'textstat_polysyllabcount', 'textstat_monosyllabcount', 'textstat_difficult_words', 'textacy_n_words', 'textacy_n_unique_words', 'textacy_n_chars_per_word_mean', 'textacy_n_chars', 'textacy_n_long_words', 'textacy_n_syllables_per_word_mean', 'textacy_n_syllables', 'textacy_n_monosyllable_words', 'textacy_n_polysyllable_words', 'textacy_entropy', 'textacy_readability_automated_readability_index', 'textacy_readability_coleman_liau_index', 'textacy_readability_flesch_kincaid_grade_level', 'textacy_readability_flesch_reading_ease', 'textacy_readability_gunning_fog_index', 'textacy_readability_smog_index', 'textacy_readability_lix', 'textacy_diversity_ttr', 'textacy_diversity_log_ttr', 'textacy_diversity_segmented_ttr', 'textacy_diversity_mtld', 'textacy_diversity_hdd', 'textacy_pos_counts_SPACE', 'textacy_dep_counts_dep', 'sentiment', 'embedding_1', 'embedding_2', 'embedding_3', 'embedding_4', 'embedding_5', 'embedding_6', 'embedding_7', 'embedding_8', 'embedding_9', 'embedding_10', 'embedding_11', 'embedding_12', 'embedding_13', 'embedding_14', 'embedding_15', 'embedding_16', 'embedding_17', 'embedding_18', 'embedding_19', 'embedding_20', 'embedding_21', 'embedding_22', 'embedding_23', 'embedding_24', 'embedding_25', 'embedding_26', 'embedding_27', 'embedding_28', 'embedding_29', 'embedding_30', 'embedding_31', 'embedding_32', 'embedding_33', 'embedding_34', 'embedding_35', 'embedding_36', 'embedding_37', 'embedding_38', 'embedding_39', 'embedding_40', 'embedding_41', 'embedding_42', 'embedding_43', 'embedding_44', 'embedding_45', 'embedding_46', 'embedding_47', 'embedding_48', 'embedding_49', 'embedding_50', 'embedding_51', 'embedding_52', 'embedding_53', 'embedding_54', 'embedding_55', 'embedding_56', 'embedding_57', 'embedding_58', 'embedding_59', 'embedding_60', 'embedding_61', 'embedding_62', 'embedding_63', 'embedding_64', 'embedding_65', 'embedding_66', 'embedding_67', 'embedding_68', 'embedding_69', 'embedding_70', 'embedding_71', 'embedding_72', 'embedding_73', 'embedding_74', 'embedding_75', 'embedding_76', 'embedding_77', 'embedding_78', 'embedding_79', 'embedding_80', 'embedding_81', 'embedding_82', 'embedding_83', 'embedding_84', 'embedding_85', 'embedding_86', 'embedding_87', 'embedding_88', 'embedding_89', 'embedding_90', 'embedding_91', 'embedding_92', 'embedding_93', 'embedding_94', 'embedding_95', 'embedding_96', 'embedding_97', 'embedding_98', 'embedding_99', 'embedding_100', 'embedding_101', 'embedding_102', 'embedding_103', 'embedding_104', 'embedding_105', 'embedding_106', 'embedding_107', 'embedding_108', 'embedding_109', 'embedding_110', 'embedding_111', 'embedding_112', 'embedding_113', 'embedding_114', 'embedding_115', 'embedding_116', 'embedding_117', 'embedding_118', 'embedding_119', 'embedding_120', 'embedding_121', 'embedding_122', 'embedding_123', 'embedding_124', 'embedding_125', 'embedding_126', 'embedding_127', 'embedding_128', 'embedding_129', 'embedding_130', 'embedding_131', 'embedding_132', 'embedding_133', 'embedding_134', 'embedding_135', 'embedding_136', 'embedding_137', 'embedding_138', 'embedding_139', 'embedding_140', 'embedding_141', 'embedding_142', 'embedding_143', 'embedding_144', 'embedding_145', 'embedding_146', 'embedding_147', 'embedding_148', 'embedding_149', 'embedding_150', 'embedding_151', 'embedding_152', 'embedding_153', 'embedding_154', 'embedding_155', 'embedding_156', 'embedding_157', 'embedding_158', 'embedding_159', 'embedding_160', 'embedding_161', 'embedding_162', 'embedding_163', 'embedding_164', 'embedding_165', 'embedding_166', 'embedding_167', 'embedding_168', 'embedding_169', 'embedding_170', 'embedding_171', 'embedding_172', 'embedding_173', 'embedding_174', 'embedding_175', 'embedding_176', 'embedding_177', 'embedding_178', 'embedding_179', 'embedding_180', 'embedding_181', 'embedding_182', 'embedding_183', 'embedding_184', 'embedding_185', 'embedding_186', 'embedding_187', 'embedding_188', 'embedding_189', 'embedding_190', 'embedding_191', 'embedding_192', 'embedding_193', 'embedding_194', 'embedding_195', 'embedding_196', 'embedding_197', 'embedding_198', 'embedding_199', 'embedding_200', 'embedding_201', 'embedding_202', 'embedding_203', 'embedding_204', 'embedding_205', 'embedding_206', 'embedding_207', 'embedding_208', 'embedding_209', 'embedding_210', 'embedding_211', 'embedding_212', 'embedding_213', 'embedding_214', 'embedding_215', 'embedding_216', 'embedding_217', 'embedding_218', 'embedding_219', 'embedding_220', 'embedding_221', 'embedding_222', 'embedding_223', 'embedding_224', 'embedding_225', 'embedding_226', 'embedding_227', 'embedding_228', 'embedding_229', 'embedding_230', 'embedding_231', 'embedding_232', 'embedding_233', 'embedding_234', 'embedding_235', 'embedding_236', 'embedding_237', 'embedding_238', 'embedding_239', 'embedding_240', 'embedding_241', 'embedding_242', 'embedding_243', 'embedding_244', 'embedding_245', 'embedding_246', 'embedding_247', 'embedding_248', 'embedding_249', 'embedding_250', 'embedding_251', 'embedding_252', 'embedding_253', 'embedding_254', 'embedding_255', 'embedding_256', 'embedding_257', 'embedding_258', 'embedding_259', 'embedding_260', 'embedding_261', 'embedding_262', 'embedding_263', 'embedding_264', 'embedding_265', 'embedding_266', 'embedding_267', 'embedding_268', 'embedding_269', 'embedding_270', 'embedding_271', 'embedding_272', 'embedding_273', 'embedding_274', 'embedding_275', 'embedding_276', 'embedding_277', 'embedding_278', 'embedding_279', 'embedding_280', 'embedding_281', 'embedding_282', 'embedding_283', 'embedding_284', 'embedding_285', 'embedding_286', 'embedding_287', 'embedding_288', 'embedding_289', 'embedding_290', 'embedding_291', 'embedding_292', 'embedding_293', 'embedding_294', 'embedding_295', 'embedding_296', 'embedding_297', 'embedding_298', 'embedding_299', 'embedding_300', 'embedding_301', 'embedding_302', 'embedding_303', 'embedding_304', 'embedding_305', 'embedding_306', 'embedding_307', 'embedding_308', 'embedding_309', 'embedding_310', 'embedding_311', 'embedding_312', 'embedding_313', 'embedding_314', 'embedding_315', 'embedding_316', 'embedding_317', 'embedding_318', 'embedding_319', 'embedding_320', 'embedding_321', 'embedding_322', 'embedding_323', 'embedding_324', 'embedding_325', 'embedding_326', 'embedding_327', 'embedding_328', 'embedding_329', 'embedding_330', 'embedding_331', 'embedding_332', 'embedding_333', 'embedding_334', 'embedding_335', 'embedding_336', 'embedding_337', 'embedding_338', 'embedding_339', 'embedding_340', 'embedding_341', 'embedding_342', 'embedding_343', 'embedding_344', 'embedding_345', 'embedding_346', 'embedding_347', 'embedding_348', 'embedding_349', 'embedding_350', 'embedding_351', 'embedding_352', 'embedding_353', 'embedding_354', 'embedding_355', 'embedding_356', 'embedding_357', 'embedding_358', 'embedding_359', 'embedding_360', 'embedding_361', 'embedding_362', 'embedding_363', 'embedding_364', 'embedding_365', 'embedding_366', 'embedding_367', 'embedding_368', 'embedding_369', 'embedding_370', 'embedding_371', 'embedding_372', 'embedding_373', 'embedding_374', 'embedding_375', 'embedding_376', 'embedding_377', 'embedding_378', 'embedding_379', 'embedding_380', 'embedding_381', 'embedding_382', 'embedding_383', 'embedding_384', 'embedding_385', 'embedding_386', 'embedding_387', 'embedding_388', 'embedding_389', 'embedding_390', 'embedding_391', 'embedding_392', 'embedding_393', 'embedding_394', 'embedding_395', 'embedding_396', 'embedding_397', 'embedding_398', 'embedding_399', 'embedding_400', 'embedding_401', 'embedding_402', 'embedding_403', 'embedding_404', 'embedding_405', 'embedding_406', 'embedding_407', 'embedding_408', 'embedding_409', 'embedding_410', 'embedding_411', 'embedding_412', 'embedding_413', 'embedding_414', 'embedding_415', 'embedding_416', 'embedding_417', 'embedding_418', 'embedding_419', 'embedding_420', 'embedding_421', 'embedding_422', 'embedding_423', 'embedding_424', 'embedding_425', 'embedding_426', 'embedding_427', 'embedding_428', 'embedding_429', 'embedding_430', 'embedding_431', 'embedding_432', 'embedding_433', 'embedding_434', 'embedding_435', 'embedding_436', 'embedding_437', 'embedding_438', 'embedding_439', 'embedding_440', 'embedding_441', 'embedding_442', 'embedding_443', 'embedding_444', 'embedding_445', 'embedding_446', 'embedding_447', 'embedding_448', 'embedding_449', 'embedding_450', 'embedding_451', 'embedding_452', 'embedding_453', 'embedding_454', 'embedding_455', 'embedding_456', 'embedding_457', 'embedding_458', 'embedding_459', 'embedding_460', 'embedding_461', 'embedding_462', 'embedding_463', 'embedding_464', 'embedding_465', 'embedding_466', 'embedding_467', 'embedding_468', 'embedding_469', 'embedding_470', 'embedding_471', 'embedding_472', 'embedding_473', 'embedding_474', 'embedding_475', 'embedding_476', 'embedding_477', 'embedding_478', 'embedding_479', 'embedding_480', 'embedding_481', 'embedding_482', 'embedding_483', 'embedding_484', 'embedding_485', 'embedding_486', 'embedding_487', 'embedding_488', 'embedding_489', 'embedding_490', 'embedding_491', 'embedding_492', 'embedding_493', 'embedding_494', 'embedding_495', 'embedding_496', 'embedding_497', 'embedding_498', 'embedding_499', 'embedding_500', 'embedding_501', 'embedding_502', 'embedding_503', 'embedding_504', 'embedding_505', 'embedding_506', 'embedding_507', 'embedding_508', 'embedding_509', 'embedding_510', 'embedding_511', 'embedding_512', 'embedding_513', 'embedding_514', 'embedding_515', 'embedding_516', 'embedding_517', 'embedding_518', 'embedding_519', 'embedding_520', 'embedding_521', 'embedding_522', 'embedding_523', 'embedding_524', 'embedding_525', 'embedding_526', 'embedding_527', 'embedding_528', 'embedding_529', 'embedding_530', 'embedding_531', 'embedding_532', 'embedding_533', 'embedding_534', 'embedding_535', 'embedding_536', 'embedding_537', 'embedding_538', 'embedding_539', 'embedding_540', 'embedding_541', 'embedding_542', 'embedding_543', 'embedding_544', 'embedding_545', 'embedding_546', 'embedding_547', 'embedding_548', 'embedding_549', 'embedding_550', 'embedding_551', 'embedding_552', 'embedding_553', 'embedding_554', 'embedding_555', 'embedding_556', 'embedding_557', 'embedding_558', 'embedding_559', 'embedding_560', 'embedding_561', 'embedding_562', 'embedding_563', 'embedding_564', 'embedding_565', 'embedding_566', 'embedding_567', 'embedding_568', 'embedding_569', 'embedding_570', 'embedding_571', 'embedding_572', 'embedding_573', 'embedding_574', 'embedding_575', 'embedding_576', 'embedding_577', 'embedding_578', 'embedding_579', 'embedding_580', 'embedding_581', 'embedding_582', 'embedding_583', 'embedding_584', 'embedding_585', 'embedding_586', 'embedding_587', 'embedding_588', 'embedding_589', 'embedding_590', 'embedding_591', 'embedding_592', 'embedding_593', 'embedding_594', 'embedding_595', 'embedding_596', 'embedding_597', 'embedding_598', 'embedding_599', 'embedding_600', 'embedding_601', 'embedding_602', 'embedding_603', 'embedding_604', 'embedding_605', 'embedding_606', 'embedding_607', 'embedding_608', 'embedding_609', 'embedding_610', 'embedding_611', 'embedding_612', 'embedding_613', 'embedding_614', 'embedding_615', 'embedding_616', 'embedding_617', 'embedding_618', 'embedding_619', 'embedding_620', 'embedding_621', 'embedding_622', 'embedding_623', 'embedding_624', 'embedding_625', 'embedding_626', 'embedding_627', 'embedding_628', 'embedding_629', 'embedding_630', 'embedding_631', 'embedding_632', 'embedding_633', 'embedding_634', 'embedding_635', 'embedding_636', 'embedding_637', 'embedding_638', 'embedding_639', 'embedding_640', 'embedding_641', 'embedding_642', 'embedding_643', 'embedding_644', 'embedding_645', 'embedding_646', 'embedding_647', 'embedding_648', 'embedding_649', 'embedding_650', 'embedding_651', 'embedding_652', 'embedding_653', 'embedding_654', 'embedding_655', 'embedding_656', 'embedding_657', 'embedding_658', 'embedding_659', 'embedding_660', 'embedding_661', 'embedding_662', 'embedding_663', 'embedding_664', 'embedding_665', 'embedding_666', 'embedding_667', 'embedding_668', 'embedding_669', 'embedding_670', 'embedding_671', 'embedding_672', 'embedding_673', 'embedding_674', 'embedding_675', 'embedding_676', 'embedding_677', 'embedding_678', 'embedding_679', 'embedding_680', 'embedding_681', 'embedding_682', 'embedding_683', 'embedding_684', 'embedding_685', 'embedding_686', 'embedding_687', 'embedding_688', 'embedding_689', 'embedding_690', 'embedding_691', 'embedding_692', 'embedding_693', 'embedding_694', 'embedding_695', 'embedding_696', 'embedding_697', 'embedding_698', 'embedding_699', 'embedding_700', 'embedding_701', 'embedding_702', 'embedding_703', 'embedding_704', 'embedding_705', 'embedding_706', 'embedding_707', 'embedding_708', 'embedding_709', 'embedding_710', 'embedding_711', 'embedding_712', 'embedding_713', 'embedding_714', 'embedding_715', 'embedding_716', 'embedding_717', 'embedding_718', 'embedding_719', 'embedding_720', 'embedding_721', 'embedding_722', 'embedding_723', 'embedding_724', 'embedding_725', 'embedding_726', 'embedding_727', 'embedding_728', 'embedding_729', 'embedding_730', 'embedding_731', 'embedding_732', 'embedding_733', 'embedding_734', 'embedding_735', 'embedding_736', 'embedding_737', 'embedding_738', 'embedding_739', 'embedding_740', 'embedding_741', 'embedding_742', 'embedding_743', 'embedding_744', 'embedding_745', 'embedding_746', 'embedding_747', 'embedding_748', 'embedding_749', 'embedding_750', 'embedding_751', 'embedding_752', 'embedding_753', 'embedding_754', 'embedding_755', 'embedding_756', 'embedding_757', 'embedding_758', 'embedding_759', 'embedding_760', 'embedding_761', 'embedding_762', 'embedding_763', 'embedding_764', 'embedding_765', 'embedding_766', 'embedding_767', 'embedding_768', 'embedding_769', 'embedding_770', 'embedding_771', 'embedding_772', 'embedding_773', 'embedding_774', 'embedding_775', 'embedding_776', 'embedding_777', 'embedding_778', 'embedding_779', 'embedding_780', 'embedding_781', 'embedding_782', 'embedding_783', 'embedding_784', 'embedding_785', 'embedding_786', 'embedding_787', 'embedding_788', 'embedding_789', 'embedding_790', 'embedding_791', 'embedding_792', 'embedding_793', 'embedding_794', 'embedding_795', 'embedding_796', 'embedding_797', 'embedding_798', 'embedding_799', 'embedding_800', 'embedding_801', 'embedding_802', 'embedding_803', 'embedding_804', 'embedding_805', 'embedding_806', 'embedding_807', 'embedding_808', 'embedding_809', 'embedding_810', 'embedding_811', 'embedding_812', 'embedding_813', 'embedding_814', 'embedding_815', 'embedding_816', 'embedding_817', 'embedding_818', 'embedding_819', 'embedding_820', 'embedding_821', 'embedding_822', 'embedding_823', 'embedding_824', 'embedding_825', 'embedding_826', 'embedding_827', 'embedding_828', 'embedding_829', 'embedding_830', 'embedding_831', 'embedding_832', 'embedding_833', 'embedding_834', 'embedding_835', 'embedding_836', 'embedding_837', 'embedding_838', 'embedding_839', 'embedding_840', 'embedding_841', 'embedding_842', 'embedding_843', 'embedding_844', 'embedding_845', 'embedding_846', 'embedding_847', 'embedding_848', 'embedding_849', 'embedding_850', 'embedding_851', 'embedding_852', 'embedding_853', 'embedding_854', 'embedding_855', 'embedding_856', 'embedding_857', 'embedding_858', 'embedding_859', 'embedding_860', 'embedding_861', 'embedding_862', 'embedding_863', 'embedding_864', 'embedding_865', 'embedding_866', 'embedding_867', 'embedding_868', 'embedding_869', 'embedding_870', 'embedding_871', 'embedding_872', 'embedding_873', 'embedding_874', 'embedding_875', 'embedding_876', 'embedding_877', 'embedding_878', 'embedding_879', 'embedding_880', 'embedding_881', 'embedding_882', 'embedding_883', 'embedding_884', 'embedding_885', 'embedding_886', 'embedding_887', 'embedding_888', 'embedding_889', 'embedding_890', 'embedding_891', 'embedding_892', 'embedding_893', 'embedding_894', 'embedding_895', 'embedding_896', 'embedding_897', 'embedding_898', 'embedding_899', 'embedding_900', 'embedding_901', 'embedding_902', 'embedding_903', 'embedding_904', 'embedding_905', 'embedding_906', 'embedding_907', 'embedding_908', 'embedding_909', 'embedding_910', 'embedding_911', 'embedding_912', 'embedding_913', 'embedding_914', 'embedding_915', 'embedding_916', 'embedding_917', 'embedding_918', 'embedding_919', 'embedding_920', 'embedding_921', 'embedding_922', 'embedding_923', 'embedding_924', 'embedding_925', 'embedding_926', 'embedding_927', 'embedding_928', 'embedding_929', 'embedding_930', 'embedding_931', 'embedding_932', 'embedding_933', 'embedding_934', 'embedding_935', 'embedding_936', 'embedding_937', 'embedding_938', 'embedding_939', 'embedding_940', 'embedding_941', 'embedding_942', 'embedding_943', 'embedding_944', 'embedding_945', 'embedding_946', 'embedding_947', 'embedding_948', 'embedding_949', 'embedding_950', 'embedding_951', 'embedding_952', 'embedding_953', 'embedding_954', 'embedding_955', 'embedding_956', 'embedding_957', 'embedding_958', 'embedding_959', 'embedding_960', 'embedding_961', 'embedding_962', 'embedding_963', 'embedding_964', 'embedding_965', 'embedding_966', 'embedding_967', 'embedding_968', 'embedding_969', 'embedding_970', 'embedding_971', 'embedding_972', 'embedding_973', 'embedding_974', 'embedding_975', 'embedding_976', 'embedding_977', 'embedding_978', 'embedding_979', 'embedding_980', 'embedding_981', 'embedding_982', 'embedding_983', 'embedding_984', 'embedding_985', 'embedding_986', 'embedding_987', 'embedding_988', 'embedding_989', 'embedding_990', 'embedding_991', 'embedding_992', 'embedding_993', 'embedding_994', 'embedding_995', 'embedding_996', 'embedding_997', 'embedding_998', 'embedding_999', 'embedding_1000', 'embedding_1001', 'embedding_1002', 'embedding_1003', 'embedding_1004', 'embedding_1005', 'embedding_1006', 'embedding_1007', 'embedding_1008', 'embedding_1009', 'embedding_1010', 'embedding_1011', 'embedding_1012', 'embedding_1013', 'embedding_1014', 'embedding_1015', 'embedding_1016', 'embedding_1017', 'embedding_1018', 'embedding_1019', 'embedding_1020', 'embedding_1021', 'embedding_1022', 'embedding_1023', 'embedding_1024']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "010c65bc2d174b53b4a1dad4ddc59bff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1), Label(value='0 / 1'))), HBox(c\u2026"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "ranks"
      ],
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 7.80720096, 14.27790063,  7.04157306, 13.82798863,  4.9874075 ,\n",
              "        3.27152934])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "execution_count": null
    }
  ]
}
