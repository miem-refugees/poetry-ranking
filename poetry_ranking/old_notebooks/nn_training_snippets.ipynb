{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {},
  "cells": [
    {
      "metadata": {},
      "source": [
        "X = df_train_features.drop(\"target\", axis=1)\n",
        "y = df_train_features[\"target\"]\n",
        "\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "sc = StandardScaler()\n",
        "\n",
        "\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_val = sc.transform(X_val)\n",
        "\n",
        "target_sc = StandardScaler()\n",
        "y_train = target_sc.fit_transform(y_train.to_numpy().reshape(-1, 1))\n",
        "y_val = target_sc.transform(y_val.to_numpy().reshape(-1, 1))\n",
        "\n",
        "\n",
        "# Assuming X_train, X_val, y_train, y_val are your data frames\n",
        "# Convert pandas dataframes to torch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
        "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
        "\n",
        "# Create DataLoader\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=512)\n",
        "\n",
        "\n",
        "# Define a simple neural network model\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_size, 1000, bias=False),\n",
        "            nn.BatchNorm1d(1000),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(1000, 2000, bias=False),\n",
        "            nn.BatchNorm1d(2000),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(2000, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "input_size = X_train.shape[1]\n",
        "model = SimpleNN(input_size)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# Training loop\n",
        "n_epochs = 10\n",
        "for epoch in trange(n_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    for X_batch, y_batch in tqdm(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(\n",
        "            outputs, y_batch.view(-1, 1)\n",
        "        )  # Reshape y_batch for compatibility\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * X_batch.size(0)\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "\n",
        "    # Validation step\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in tqdm(val_loader):\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch.view(-1, 1))\n",
        "            val_loss += loss.item() * X_batch.size(0)\n",
        "\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch+1}/{n_epochs}, Training Loss: {np.sqrt(train_loss):.4f}, Validation Loss: {np.sqrt(val_loss):.4f}\"\n",
        "    )"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Collect predictions for the entire validation set\n",
        "all_predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_batch, _ in val_loader:\n",
        "        outputs = model(X_batch)\n",
        "        all_predictions.append(\n",
        "            outputs.squeeze().cpu().numpy()\n",
        "        )  # Squeeze to remove extra dimensions\n",
        "\n",
        "y_pred = np.concatenate(all_predictions)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "y_pred = target_sc.inverse_transform(y_pred.reshape(-1, 1)).reshape(1, -1).squeeze()\n",
        "y_val = target_sc.inverse_transform(y_val.reshape(-1, 1)).reshape(1, -1).squeeze()\n",
        "kendalltau(y_val, y_pred)\n",
        "kendalltau(y_val, y_pred)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    }
  ]
}
